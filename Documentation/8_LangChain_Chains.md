
# ðŸ“Œ **Chains in LangChain**  

### ðŸ§ **1. What and Why is Chains?**  

In **LangChain**, a **Chain** is a sequence of steps where the output of one step becomes the input of another. This helps in structuring complex LLM workflows efficiently.  

ðŸ”¹ **Why use Chains?**  
âœ… Simplifies multi-step reasoning.  
âœ… Reduces redundant code by reusing components.  
âœ… Improves modularity and debugging.  

LangChain provides different types of chains to handle various workflows. Letâ€™s explore them!  

---

## ðŸ”— **2. Simple Chain**  

A **Simple Chain** consists of a **Prompt**, a **Language Model**, and an **Output Parser** connected in a sequence.  

ðŸ’¡ **Example:**  

```python
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI
from langchain.chains import LLMChain

# Define the prompt
prompt = PromptTemplate.from_template("What is the capital of {country}?")

# Create a simple chain
llm = OpenAI(model="gpt-4")
chain = LLMChain(prompt=prompt, llm=llm)

# Run the chain
response = chain.run({"country": "France"})
print(response)  # Output: "Paris"
```

ðŸ“Œ **Breakdown:**  
- Takes `country` as input.  
- Fills the template and queries the LLM.  
- Returns the modelâ€™s response.  

---

## ðŸ”„ **3. Sequential Chain**  

A **Sequential Chain** is when multiple chains run in **series**, where the output of one chain is passed as the input to the next.  

ðŸ›  **Use Case:** Multi-step reasoning, dialogue generation, research automation.  

ðŸ’¡ **Example:**  

```python
from langchain.chains import SimpleSequentialChain

# Define two chains
first_chain = LLMChain(prompt=PromptTemplate.from_template("Who is {person}?"), llm=llm)
second_chain = LLMChain(prompt=PromptTemplate.from_template("Summarize: {info}"), llm=llm)

# Create a sequential chain
chain = SimpleSequentialChain(chains=[first_chain, second_chain])

# Run the chain
result = chain.run({"person": "Elon Musk"})
print(result)
```

ðŸ“Œ **Flow:**  
1ï¸âƒ£ First chain fetches information about *Elon Musk*.  
2ï¸âƒ£ Second chain summarizes that information.  

---

## âš¡ **4. Parallel Chain**  

A **Parallel Chain** allows multiple chains to execute **simultaneously** and return results together.  

ðŸ›  **Use Case:** Running multiple LLM queries independently.  

ðŸ’¡ **Example:**  

```python
from langchain.chains import RunnableParallel

# Define multiple chains
capital_chain = LLMChain(prompt=PromptTemplate.from_template("What is the capital of {country}?"), llm=llm)
language_chain = LLMChain(prompt=PromptTemplate.from_template("What language is spoken in {country}?"), llm=llm)

# Run chains in parallel
chain = RunnableParallel(capital=capital_chain, language=language_chain)
result = chain.invoke({"country": "Germany"})

print(result)  
# Output: {"capital": "Berlin", "language": "German"}
```

ðŸ“Œ **Flow:**  
- **Both chains execute independently** without waiting for each other.  
- Returns a **dictionary** containing results from all chains.  

---

## ðŸ”€ **5. Conditional Chain**  

A **Conditional Chain** allows for dynamic decision-making based on inputs.  

ðŸ›  **Use Case:** Choosing different LLMs, varying response format, conditional logic.  

ðŸ’¡ **Example:**  

```python
from langchain.schema import RunnableLambda
from langchain.chains import RunnableBranch

# Define conditions
def choose_chain(input):
    return "long_chain" if len(input["text"]) > 50 else "short_chain"

short_chain = LLMChain(prompt=PromptTemplate.from_template("Summarize in one sentence: {text}"), llm=llm)
long_chain = LLMChain(prompt=PromptTemplate.from_template("Write a detailed summary: {text}"), llm=llm)

# Create conditional chain
chain = RunnableBranch(
    (lambda x: len(x["text"]) > 50, long_chain),
    (lambda x: True, short_chain),  # Default case
)

# Run the chain
result = chain.invoke({"text": "The quick brown fox jumps over the lazy dog."})
print(result)
```

ðŸ“Œ **Flow:**  
- If the input text is long, the **long_chain** executes.  
- If short, the **short_chain** executes.  

---

## ðŸ— **Understanding `chain = prompt | model | parser`**  

In **LangChain**, a chain follows a pipeline structure:  

**ðŸ“Œ `chain = prompt | model | parser`**  

ðŸ”¹ **Prompt:** Defines how the input is structured.  
ðŸ”¹ **Model:** Processes the prompt using an LLM.  
ðŸ”¹ **Parser:** Parses and formats the LLM output.  

ðŸ’¡ **Example:**  

```python
from langchain.output_parsers import StrOutputParser

prompt = PromptTemplate.from_template("Who is {name}?")
parser = StrOutputParser()

chain = prompt | llm | parser  # Equivalent to a full chain

response = chain.invoke({"name": "Albert Einstein"})
print(response)  
```

ðŸ“Œ **Flow:**  
1ï¸âƒ£ **Prompt** formats input.  
2ï¸âƒ£ **LLM** processes the prompt.  
3ï¸âƒ£ **Parser** cleans and structures the output.  

---

## ðŸ“Š **Visualizing Chain Execution with `get_graph().print_ascii()`**  

LangChain allows **visualizing** the structure of a chain using ASCII diagrams.  

ðŸ’¡ **Example:**  

```python
chain.get_graph().print_ascii()
```

ðŸ” **Output (for a sequential chain):**  

```
   (input)
      |
  LLMChain  --> LLMChain
      |
   (output)
```

ðŸ“Œ **Benefits:**  
âœ… Helps in debugging.  
âœ… Shows dependencies between chains.  
âœ… Useful for complex workflows.  

---

## ðŸŽ¯ **Conclusion**  

Chains in LangChain provide a **structured** and **modular** way to build LLM-powered applications.  

ðŸ’¡ **Key Takeaways:**  
âœ… **Simple Chain** â€“ Directly maps input â†’ output.  
âœ… **Sequential Chain** â€“ Passes output from one chain to another.  
âœ… **Parallel Chain** â€“ Runs multiple chains simultaneously.  
âœ… **Conditional Chain** â€“ Executes chains based on conditions.  
âœ… **Pipeline Model** â€“ `prompt | model | parser`.  
âœ… **Graph Visualization** â€“ Helps debug and optimize chains.  

---

